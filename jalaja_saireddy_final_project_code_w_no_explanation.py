# -*- coding: utf-8 -*-
"""Jalaja_Saireddy_Final_Project_CODE_W_NO_EXPLANATION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ed250AbGTZmjlOs8jXvyp21nCCH0iesu
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb

df = pd.read_csv('NBA_Dataset.csv')

df_filtered = df[(df['season'] >= 1982) & (df['season'] <= 2022)]

df_filtered['is_mvp'] = df_filtered.groupby('season')['award_share'].transform('max') == df_filtered['award_share']
df_filtered['is_mvp'] = df_filtered['is_mvp'].astype(int)

df_filtered.head()

df_filtered.info()

mvp_players = df_filtered[df_filtered['is_mvp'] == 1]
mvp_players

mvp_counts = mvp_players['player'].value_counts()

plt.figure(figsize=(10, 6))
mvp_counts.plot(kind='bar')
plt.xlabel('Player')
plt.ylabel('Number of MVPs')
plt.title('Number of MVP Awards Per Player')
plt

numeric_df = df_filtered.select_dtypes(include=['number'])

correlation_matrix = numeric_df.corr()
correlation_matrix = correlation_matrix.round(1)

plt.figure(figsize=(22, 18))
sns.heatmap(correlation_matrix, annot=True, cmap='plasma', fmt='.1f', linewidths=2,
            annot_kws={'size': 10}, cbar_kws={'shrink': 0.8})
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.title('Correlation Matrix of Features', fontsize=20)
plt.tight_layout()
plt.show()

df_filtered = df_filtered.drop(columns=['player', 'team_id', 'fg_per_g', 'fga_per_g', 'fg3_per_g', 'fg3a_per_g', 'fg2_per_g', 'fg2a_per_g', 'ft_per_g', 'fta_per_g', 'ws', 'mov', 'mov_adj'])

numeric_df = df_filtered.select_dtypes(include=['number'])

correlation_matrix = numeric_df.corr()
correlation_matrix = correlation_matrix.round(1)

plt.figure(figsize=(16.5, 13.5))
sns.heatmap(correlation_matrix, annot=True, cmap='plasma', fmt='.1f', linewidths=1,
            annot_kws={'size': 10}, cbar_kws={'shrink': 0.8})
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.yticks(rotation=0, fontsize=10)
plt.title('Correlation Matrix of Features', fontsize=20)
plt.tight_layout()
plt.show()

#cleaning data before regression
df = df_filtered.dropna()

#dropping award share and is_mvp as the share of MVP votes will obviously have a strong correlation with MVP win and position because it is non-numerical but still in the data
X = df.drop(columns=['award_share', 'pos', 'is_mvp'])
y = df['is_mvp']

# initalizing the linear regression model
model = LinearRegression()

#storing features and their coefficients from the linear regression
feature_coefficients = []
for feature in X.columns:
      X_feature = X[[feature]].values
      model.fit(X_feature, y)
      feature_coefficients.append({
          'Feature': feature,
          'Coefficient': model.coef_[0]
      })

#taking the absolute value of coefficients and sorting them
if feature_coefficients:
    coef_df = pd.DataFrame(feature_coefficients)
    coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()
    coef_df_sorted = coef_df.sort_values(by='Abs_Coefficient', ascending=False)

# plottig the feature importances (absolute value of coefficients)
    plt.figure(figsize=(12, 8))
    sns.barplot(x='Abs_Coefficient', y='Feature', data=coef_df_sorted)
    plt.title('Ranking of Features by Absolute Coefficient for MVP Prediction')
    plt.xlabel('Absolute Coefficient')
    plt.ylabel('Feature')
    plt.show()

# filtering numeric data and dropping null values
df_filtered = df_filtered.select_dtypes(include=[np.number]).dropna()

# removing the 'award_share' column, as it implicitly defines the is_mvp column
df_filtered = df_filtered.drop(columns=['award_share'])

X = df_filtered.drop(columns=['is_mvp'])

# target variable
y = df_filtered['is_mvp']

# splitting the data into train (1982-2002) and test (2003-2022)
train_df = df_filtered[df_filtered['season'] <= 2002]
test_df = df_filtered[df_filtered['season'] > 2002]

# defining features and target for train and test sets
X_train = train_df.drop(columns=['is_mvp'])
y_train = train_df['is_mvp']
X_test = test_df.drop(columns=['is_mvp'])
y_test = test_df['is_mvp']

# scaling the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Logistic Regression Model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)
y_pred_log_reg_test = log_reg.predict(X_test_scaled)
y_pred_log_reg_train = log_reg.predict(X_train_scaled)

# Accuracy Scores
log_reg_train_acc = accuracy_score(y_train, y_pred_log_reg_train)
log_reg_test_acc = accuracy_score(y_test, y_pred_log_reg_test)

print(f"Logistic Regression Train Accuracy: {log_reg_train_acc:.5f}")
print(f"Logistic Regression Test Accuracy: {log_reg_test_acc:.5f}")
print()

# Confusion Matrix
print('Logistic Regression Confusion Matrix:\n', confusion_matrix(y_test, y_pred_log_reg_test))

# 2. Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)
y_pred_rf_test = rf.predict(X_test_scaled)
y_pred_rf_train = rf.predict(X_train_scaled)

# Accuracy Scores
rf_train_acc = accuracy_score(y_train, y_pred_rf_train)
rf_test_acc = accuracy_score(y_test, y_pred_rf_test)

print()
print(f"Random Forest Train Accuracy: {rf_train_acc:.5f}")
print(f"Random Forest Test Accuracy: {rf_test_acc:.5f}")
print()

# Confusion Matrix
print('Random Forest Confusion Matrix:\n', confusion_matrix(y_test, y_pred_rf_test))

# 3. XGBoost Classifier
xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)
xgb_model.fit(X_train_scaled, y_train)
y_pred_xgb_test = xgb_model.predict(X_test_scaled)
y_pred_xgb_train = xgb_model.predict(X_train_scaled)

# Accuracy Scores
xgb_train_acc = accuracy_score(y_train, y_pred_xgb_train)
xgb_test_acc = accuracy_score(y_test, y_pred_xgb_test)

print()
print(f"XGBoost Train Accuracy: {xgb_train_acc:.5f}")
print(f"XGBoost Test Accuracy: {xgb_test_acc:.5f}")
print()

# Confusion Matrix
print('XGBoost Confusion Matrix:\n', confusion_matrix(y_test, y_pred_xgb_test))

# Plotting Confusion Matrices
def plot_conf_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='plasma', xticklabels=['Not MVP', 'MVP'], yticklabels=['Not MVP', 'MVP'])
    plt.title(f'{model_name} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

plot_conf_matrix(y_test, y_pred_log_reg_test, 'Logistic Regression')
plot_conf_matrix(y_test, y_pred_rf_test, 'Random Forest')
plot_conf_matrix(y_test, y_pred_xgb_test, 'XGBoost')

#Visualizing feature importance for the above 3 models

# Logistic Regression: Feature Importance
log_reg_coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': log_reg.coef_[0]  # Get the coefficients for the features
}).sort_values(by='Coefficient', ascending=False)

# Plotting
plt.figure(figsize=(12, 8))
sns.barplot(x='Coefficient', y='Feature', data=log_reg_coefficients)
plt.title('Logistic Regression Coefficients')
plt.show()

# Random Forest: Feature Importance
rf_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf.feature_importances_  # Get feature importances from Random Forest
}).sort_values(by='Importance', ascending=False)

# Plotting
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=rf_importances)
plt.title('Random Forest Feature Importances')
plt.show()

# XGBoost: Feature Importance
xgb_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': xgb_model.feature_importances_  # Get feature importances from XGBoost
}).sort_values(by='Importance', ascending=False)

# Plotting
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=xgb_importances)
plt.title('XGBoost Feature Importances')
plt.show()

# filtering numeric data and dropping null values
df_filtered = df_filtered.select_dtypes(include=[np.number]).dropna()

# removing the 'award_share' column, as it implicitly defines the is_mvp column
df_filtered = df_filtered.drop(columns=['award_share'])

X = df_filtered.drop(columns=['is_mvp'])

# target variable
y = df_filtered['is_mvp']

# splitting the data into train (even years) and test (odd years)
train_df = df_filtered[df_filtered['season'] % 2 == 0]  # Even years for training
test_df = df_filtered[df_filtered['season'] % 2 != 0]   # Odd years for testing

# defining features and target for train and test sets
X_train = train_df.drop(columns=['is_mvp'])
y_train = train_df['is_mvp']
X_test = test_df.drop(columns=['is_mvp'])
y_test = test_df['is_mvp']

# scaling the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression Model
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)
y_pred_log_reg_test = log_reg.predict(X_test_scaled)
y_pred_log_reg_train = log_reg.predict(X_train_scaled)

# Accuracy Scores
log_reg_train_acc = accuracy_score(y_train, y_pred_log_reg_train)
log_reg_test_acc = accuracy_score(y_test, y_pred_log_reg_test)

print(f"Logistic Regression Train Accuracy: {log_reg_train_acc:.5f}")
print(f"Logistic Regression Test Accuracy: {log_reg_test_acc:.5f}")

# Confusion Matrix
print('Logistic Regression Confusion Matrix:\n', confusion_matrix(y_test, y_pred_log_reg_test))

# 2. Random Forest Classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)
y_pred_rf_test = rf.predict(X_test_scaled)
y_pred_rf_train = rf.predict(X_train_scaled)

# Accuracy Scores
rf_train_acc = accuracy_score(y_train, y_pred_rf_train)
rf_test_acc = accuracy_score(y_test, y_pred_rf_test)

print()
print(f"Random Forest Train Accuracy: {rf_train_acc:.5f}")
print(f"Random Forest Test Accuracy: {rf_test_acc:.5f}")

# Confusion Matrix
print('Random Forest Confusion Matrix:\n', confusion_matrix(y_test, y_pred_rf_test))

# 3. XGBoost Classifier
xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)
xgb_model.fit(X_train_scaled, y_train)
y_pred_xgb_test = xgb_model.predict(X_test_scaled)
y_pred_xgb_train = xgb_model.predict(X_train_scaled)

# Accuracy Scores
xgb_train_acc = accuracy_score(y_train, y_pred_xgb_train)
xgb_test_acc = accuracy_score(y_test, y_pred_xgb_test)

print()
print(f"XGBoost Train Accuracy: {xgb_train_acc:.5f}")
print(f"XGBoost Test Accuracy: {xgb_test_acc:.5f}")

# Confusion Matrix
print('XGBoost Confusion Matrix:\n', confusion_matrix(y_test, y_pred_xgb_test))

# Plotting Confusion Matrices
def plot_conf_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='plasma', xticklabels=['Not MVP', 'MVP'], yticklabels=['Not MVP', 'MVP'])
    plt.title(f'{model_name} Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

plot_conf_matrix(y_test, y_pred_log_reg_test, 'Logistic Regression')
plot_conf_matrix(y_test, y_pred_rf_test, 'Random Forest')
plot_conf_matrix(y_test, y_pred_xgb_test, 'XGBoost')

# Visualizing feature importance for the above 3 models

# Logistic Regression: Feature Importance
log_reg_coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': log_reg.coef_[0]  # Get the coefficients for the features
}).sort_values(by='Coefficient', ascending=False)

# Plotting Logistic Regression Coefficients
plt.figure(figsize=(12, 8))
sns.barplot(x='Coefficient', y='Feature', data=log_reg_coefficients)
plt.title('Logistic Regression Coefficients')
plt.show()

# Random Forest: Feature Importance
rf_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf.feature_importances_  # Get feature importances from Random Forest
}).sort_values(by='Importance', ascending=False)

# Plotting Random Forest Feature Importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=rf_importances)
plt.title('Random Forest Feature Importances')
plt.show()

# XGBoost: Feature Importance
xgb_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': xgb_model.feature_importances_  # Get feature importances from XGBoost
}).sort_values(by='Importance', ascending=False)

# Plotting XGBoost Feature Importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=xgb_importances)
plt.title('XGBoost Feature Importances')
plt.show()